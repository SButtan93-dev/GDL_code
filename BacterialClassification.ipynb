{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bacterial Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goggle Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "    \n",
    "%cd drive/My Drive/projects/GDL_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.loaders import load_bacteria\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation resolution - Must be square \n",
    "# Training data is also scaled to this.\n",
    "GENERATE_SQUARE = 250\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = '/content/drive/My Drive/projects/GDL_code/data/bacteria/'\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"Will generate {GENERATE_SQUARE}px square images.\")\n",
    "\n",
    "# run params\n",
    "SECTION = 'cnn'\n",
    "RUN_ID = '0001'\n",
    "DATA_NAME = 'bacteria'\n",
    "RUN_FOLDER = 'run/{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load & preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on size of image dataset, initial preprocessing can take a while.\n",
    "# Because of this time needed, save a Numpy preprocessed file.\n",
    "# In case this file is large enough to cause problems for some verisons of Pickle,\n",
    "# we use Numpy binary files instead.\n",
    "training_data = load_bacteria(DATA_PATH, GENERATE_SQUARE, GENERATE_SQUARE, IMAGE_CHANNELS, train=True)\n",
    "\n",
    "# test image loading\n",
    "plt.imshow(training_data[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImages = np.array([i[0] for i in training_data]).reshape(\n",
    "    -1, GENERATE_SQUARE, GENERATE_SQUARE, IMAGE_CHANNELS)\n",
    "trainLabels = np.array([i[1] for i in training_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "batch_norm_momentum = 0.9\n",
    "dropout_rate = 0.25\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS)), name='model_input')\n",
    "\n",
    "x = input_layer\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3, 3),\n",
    "    activation='relu',\n",
    "    strides=2,\n",
    "    padding='same',\n",
    "    kernel_initializer=weight_init)(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = BatchNormalization(momentum=batch_norm_momentum)(x)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3, 3),\n",
    "    activation='relu',\n",
    "    strides=2,\n",
    "    padding='same',\n",
    "    kernel_initializer=weight_init)(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = BatchNormalization(momentum=batch_norm_momentum)(x)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=96,\n",
    "    kernel_size=(3, 3),\n",
    "    activation='relu',\n",
    "    strides=2,\n",
    "    padding='same',\n",
    "    kernel_initializer=weight_init)(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = BatchNormalization(momentum=batch_norm_momentum)(x)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3, 3),\n",
    "    activation='relu',\n",
    "    strides=2,\n",
    "    padding='same',\n",
    "    kernel_initializer=weight_init)(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = BatchNormalization(momentum=batch_norm_momentum)(x)\n",
    "\n",
    "x = Dropout(rate=dropout_rate)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(256, activation='relu', kernel_initializer=weight_init)(x)\n",
    "\n",
    "x = Dropout(rate=dropout_rate)(x)\n",
    "\n",
    "x = Dense(128, activation='relu', kernel_initializer=weight_init)(x)\n",
    "\n",
    "output_layer = Dense(2, activation='softmax', kernel_initializer=weight_init)(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trainImages, trainLabels, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on size of image dataset, initial preprocessing can take a while.\n",
    "# Because of this time needed, save a Numpy preprocessed file.\n",
    "# In case this file is large enough to cause problems for some verisons of Pickle,\n",
    "# we use Numpy binary files instead.\n",
    "test_data = load_bacteria(DATA_PATH, GENERATE_SQUARE, GENERATE_SQUARE, IMAGE_CHANNELS, train=False)\n",
    "plt.imshow(test_data[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testImages = np.array([i[0] for i in test_data]).reshape(\n",
    "    -1, GENERATE_SQUARE, GENERATE_SQUARE, IMAGE_CHANNELS)\n",
    "testLabels = np.array([i[1] for i in test_data])\n",
    "\n",
    "loss, acc = model.evaluate(testImages, testLabels, verbose=0)\n",
    "print(acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative_fork [Python 3.7 (tensorflow 2.0.0, Keras2.3.1)]\n",
   "language": "python",
   "name": "generative_fork"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
