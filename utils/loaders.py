import pickle
import os
from tqdm import tqdm
import time

from tensorflow.keras.datasets import mnist, cifar100,cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, save_img, img_to_array

import pandas as pd
from PIL import Image
import numpy as np
from os import walk, getcwd
import h5py

import imageio
from glob import glob

from tensorflow.keras.applications import vgg19
from tensorflow.keras import backend as K
from tensorflow.keras.utils import to_categorical

import tensorflow as tf

import pdb


class ImageLabelLoader():
    def __init__(self, image_folder, target_size):
        self.image_folder = image_folder
        self.target_size = target_size

    def build(self, att, batch_size, label = None):

        data_gen = ImageDataGenerator(rescale=1./255)
        if label:
            data_flow = data_gen.flow_from_dataframe(
                att
                , self.image_folder
                , x_col='image_id'
                , y_col=label
                , target_size=self.target_size
                , class_mode='other'
                , batch_size=batch_size
                , shuffle=True
            )
        else:
            data_flow = data_gen.flow_from_dataframe(
                att
                , self.image_folder
                , x_col='image_id'
                , target_size=self.target_size
                , class_mode='input'
                , batch_size=batch_size
                , shuffle=True
            )

        return data_flow




class DataLoader():
    def __init__(self, dataset_name, img_res=(256, 256)):
        self.dataset_name = dataset_name
        self.img_res = img_res

    def load_data(self, domain, batch_size=1, is_testing=False):
        data_type = "train%s" % domain if not is_testing else "test%s" % domain
        path = glob('./data/%s/%s/*' % (self.dataset_name, data_type))

        batch_images = np.random.choice(path, size=batch_size)

        imgs = []
        for img_path in batch_images:
            img = self.imread(img_path)
            if not is_testing:
                img = np.array(Image.fromarray(img).resize(self.img_res))

                if np.random.random() > 0.5:
                    img = np.fliplr(img)
            else:
                img = np.array(Image.fromarray(img).resize(self.img_res))
            imgs.append(img)

        imgs = np.array(imgs)/127.5 - 1.

        return imgs

    def load_batch(self, batch_size=1, is_testing=False):
        data_type = "train" if not is_testing else "val"
        path_A = glob('./data/%s/%sA/*' % (self.dataset_name, data_type))
        path_B = glob('./data/%s/%sB/*' % (self.dataset_name, data_type))

        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)
        total_samples = self.n_batches * batch_size

        # Sample n_batches * batch_size from each path list so that model sees all
        # samples from both domains
        path_A = np.random.choice(path_A, total_samples, replace=False)
        path_B = np.random.choice(path_B, total_samples, replace=False)

        for i in range(self.n_batches-1):
            batch_A = path_A[i*batch_size:(i+1)*batch_size]
            batch_B = path_B[i*batch_size:(i+1)*batch_size]
            imgs_A, imgs_B = [], []
            for img_A, img_B in zip(batch_A, batch_B):
                img_A = self.imread(img_A)
                img_B = self.imread(img_B)

                img_A = np.array(Image.fromarray(img_A).resize(self.img_res))
                img_B = np.array(Image.fromarray(img_B).resize(self.img_res))

                if not is_testing and np.random.random() > 0.5:
                    img_A = np.fliplr(img_A)
                    img_B = np.fliplr(img_B)

                imgs_A.append(img_A)
                imgs_B.append(img_B)

            imgs_A = np.array(imgs_A)/127.5 - 1.
            imgs_B = np.array(imgs_B)/127.5 - 1.

            yield imgs_A, imgs_B

    def load_img(self, path):
        img = self.imread(path)
        img = np.array(Image.fromarray(img).resize(self.img_res))
        img = img/127.5 - 1.
        return img[np.newaxis, :, :, :]

    def imread(self, path):
        return imageio.imread(path, pilmode='RGB').astype(np.uint8)




def load_model(model_class, folder):

    with open(os.path.join(folder, 'params.pkl'), 'rb') as f:
        params = pickle.load(f)

    model = model_class(*params)

    model.load_weights(os.path.join(folder, 'weights/weights.h5'))

    return model


def load_mnist():
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    x_train = x_train.astype('float32') / 255.
    x_train = x_train.reshape(x_train.shape + (1,))
    x_test = x_test.astype('float32') / 255.
    x_test = x_test.reshape(x_test.shape + (1,))

    return (x_train, y_train), (x_test, y_test)

def load_mnist_gan():
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    x_train = (x_train.astype('float32') - 127.5) / 127.5
    x_train = x_train.reshape(x_train.shape + (1,))
    x_test = (x_test.astype('float32') - 127.5) / 127.5
    x_test = x_test.reshape(x_test.shape + (1,))

    return (x_train, y_train), (x_test, y_test)



def load_fashion_mnist(input_rows, input_cols, path='./data/fashion/fashion-mnist_train.csv'):
    #read the csv data
    df = pd.read_csv(path)
    #extract the image pixels
    X_train = df.drop(columns = ['label'])
    X_train = X_train.values
    X_train = (X_train.astype('float32') - 127.5) / 127.5
    X_train = X_train.reshape(X_train.shape[0], input_rows, input_cols, 1)
    #extract the labels
    y_train = df['label'].values

    return X_train, y_train

def load_safari(folder):

    mypath = os.path.join("./data", folder)
    txt_name_list = []
    for (dirpath, dirnames, filenames) in walk(mypath):
        for f in filenames:
            if f != '.DS_Store':
                txt_name_list.append(f)
                break

    slice_train = int(80000/len(txt_name_list))  ###Setting value to be 80000 for the final dataset
    i = 0
    seed = np.random.randint(1, 10e6)

    for txt_name in txt_name_list:
        txt_path = os.path.join(mypath,txt_name)
        x = np.load(txt_path)
        x = (x.astype('float32') - 127.5) / 127.5
        # x = x.astype('float32') / 255.0

        x = x.reshape(x.shape[0], 28, 28, 1)

        y = [i] * len(x)
        np.random.seed(seed)
        np.random.shuffle(x)
        np.random.seed(seed)
        np.random.shuffle(y)
        x = x[:slice_train]
        y = y[:slice_train]
        if i != 0:
            xtotal = np.concatenate((x,xtotal), axis=0)
            ytotal = np.concatenate((y,ytotal), axis=0)
        else:
            xtotal = x
            ytotal = y
        i += 1

    return xtotal, ytotal



def load_cifar(label, num):
    if num == 10:
        (x_train, y_train), (x_test, y_test) = cifar10.load_data()
    else:
        (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode = 'fine')

    train_mask = [y[0]==label for y in y_train]
    test_mask = [y[0]==label for y in y_test]

    x_data = np.concatenate([x_train[train_mask], x_test[test_mask]])
    y_data = np.concatenate([y_train[train_mask], y_test[test_mask]])

    x_data = (x_data.astype('float32') - 127.5) / 127.5

    return (x_data, y_data)


def load_celeb(data_name, image_size, batch_size):
    data_folder = os.path.join("./data", data_name)

    data_gen = ImageDataGenerator(preprocessing_function=lambda x: (x.astype('float32') - 127.5) / 127.5)

    x_train = data_gen.flow_from_directory(data_folder
                                            , target_size = (image_size,image_size)
                                            , batch_size = batch_size
                                            , shuffle = True
                                            , class_mode = 'input'
                                            , subset = "training"
                                                )

    return x_train


def load_music(data_name, filename, n_bars, n_steps_per_bar):
    file = os.path.join("./data", data_name, filename)

    with np.load(file, encoding='bytes', allow_pickle = True) as f:
        data = f['train']

    data_ints = []

    for x in data:
        counter = 0
        cont = True
        while cont:
            if not np.any(np.isnan(x[counter:(counter+4)])):
                cont = False
            else:
                counter += 4

        if n_bars * n_steps_per_bar < x.shape[0]:
            data_ints.append(x[counter:(counter + (n_bars * n_steps_per_bar)),:])


    data_ints = np.array(data_ints)

    n_songs = data_ints.shape[0]
    n_tracks = data_ints.shape[2]

    data_ints = data_ints.reshape([n_songs, n_bars, n_steps_per_bar, n_tracks])

    max_note = 83

    where_are_NaNs = np.isnan(data_ints)
    data_ints[where_are_NaNs] = max_note + 1
    max_note = max_note + 1

    data_ints = data_ints.astype(int)

    num_classes = max_note + 1


    data_binary = np.eye(num_classes)[data_ints]
    data_binary[data_binary==0] = -1
    data_binary = np.delete(data_binary, max_note,-1)

    data_binary = data_binary.transpose([0,1,2, 4,3])





    return data_binary, data_ints, data


def preprocess_image(data_name, file, img_nrows, img_ncols):

    image_path = os.path.join('./data', data_name, file)

    img = load_img(image_path, target_size=(img_nrows, img_ncols))
    img = img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = vgg19.preprocess_input(img)
    return img



def load_beauty(data_path, x_dim, y_dim, channels):
    # Image set has about 10,000 images. Can take over an hour
    # for initial preprocessing.
    # Because of this time needed, save a Numpy preprocessed file.
    # Note, that file is large enough to cause problems for some verisons of Pickle, so Numpy binary files are used.
    training_binary_path = os.path.join(
        data_path,f'beauty_training_data_{x_dim}_{y_dim}.npy')

    print(f"Looking for file: {training_binary_path}")

    if not os.path.isfile(training_binary_path):
        start = time.time()
        print("Loading training images...")

        training_data = []

        beauty_path = os.path.join(data_path,'beauty_images')
        for filename in tqdm(os.listdir(beauty_path)):
            try:
                path = os.path.join(beauty_path,filename)
                img = Image.open(path)
                if img.mode != "RGB":
                    img = img.convert("RGB")
                img = img.resize((x_dim,y_dim),Image.ANTIALIAS)
                training_data.append(np.asarray(img))
            except IOError:
                print("Error reading image! Skipping image.")
                continue

        training_data = np.reshape(training_data,(-1,x_dim,y_dim,channels))
        training_data = training_data.astype(np.float32)
        training_data = training_data / 127.5 - 1.

        print("Saving training image binary...")
        np.save(training_binary_path,training_data)
        elapsed = time.time()-start
        print (f'Image preprocess time: {hms_string(elapsed)}')
    else:
        print("Loading previous training pickle...")
        training_data = np.load(training_binary_path, allow_pickle=True)

    seed = np.random.randint(1, 10e6)
    np.random.seed(seed)
    np.random.shuffle(training_data)

    return training_data


def load_bacteria(data_path, block_w, block_h, channels, train=True, label_delim='_'):
    if train:
        # Depending on size of image dataset, initial preprocessing can take a while.
        # Because of this time needed, save a Numpy preprocessed file.
        # In case this file is large enough to cause problems for some verisons of Pickle,
        # we use Numpy binary files instead.
        training_binary_path = os.path.join(
            data_path,f'bacteria_training_data_{block_w}_{block_h}.npy')

        print(f"Looking for file: {training_binary_path}")

        if not os.path.isfile(training_binary_path):
            start = time.time()
            print("Loading training images...")

            training_data = []

            bacteria_path = os.path.join(data_path,'train')
            for filename in tqdm(os.listdir(bacteria_path)):
                try:
                    label = label_bacteria_img(filename, label_delim)
                    path = os.path.join(bacteria_path, filename)
                    img = Image.open(path)
                    if img.mode != "RGB":
                        img = img.convert("RGB")
                    # https://towardsdatascience.com/microbe-classification-using-deep-learning-e84312046334
                    imarray = np.array(img) #make a copy of the image as a numpy array
                    im_h, im_w = imarray.shape[:2]
                    bw, bh = block_w, block_h
                    for row in np.arange(im_h - bh+1, step=bh):
                        for col in np.arange(im_w - bw+1, step=bw):
                            im = imarray[row:row+bh, col:col+bw, :]
                            training_data.append([im, label])
                except IOError:
                    print("Error reading image! Skipping image.")
                    continue

            print("Saving training image binary...")
            np.save(training_binary_path, training_data)
            elapsed = time.time()-start
            print (f'Image preprocess time: {hms_string(elapsed)}')
        else:
            print("Loading previous training pickle...")
            training_data = np.load(training_binary_path, allow_pickle=True)

        seed = np.random.randint(1, 10e6)
        np.random.seed(seed)
        np.random.shuffle(training_data)

        return training_data
    else:
        # Depending on size of image dataset, initial preprocessing can take a while.
        # Because of this time needed, save a Numpy preprocessed file.
        # In case this file is large enough to cause problems for some verisons of Pickle,
        # we use Numpy binary files instead.
        test_binary_path = os.path.join(
            data_path,f'bacteria_test_data_{block_w}_{block_h}.npy')

        print(f"Looking for file: {test_binary_path}")

        if not os.path.isfile(test_binary_path):
            start = time.time()
            print("Loading test images...")

            test_data = []

            bacteria_path = os.path.join(data_path,'test')
            for filename in tqdm(os.listdir(bacteria_path)):
                try:
                    label = label_bacteria_img(filename, label_delim)
                    path = os.path.join(bacteria_path, filename)
                    img = Image.open(path)
                    if img.mode != "RGB":
                        img = img.convert("RGB")
                    # https://towardsdatascience.com/microbe-classification-using-deep-learning-e84312046334
                    imarray = np.array(img) #make a copy of the image as a numpy array
                    im_h, im_w = imarray.shape[:2]
                    bw, bh = block_w, block_h
                    for row in np.arange(im_h - bh+1, step=bh):
                        for col in np.arange(im_w - bw+1, step=bw):
                            im = imarray[row:row+bh, col:col+bw, :]
                            test_data.append([im, label])
                except IOError:
                    print("Error reading image! Skipping image.")
                    continue

            print("Saving test image binary...")
            np.save(test_binary_path, test_data)
            elapsed = time.time()-start
            print (f'Image preprocess time: {hms_string(elapsed)}')
        else:
            print("Loading previous testing pickle...")
            test_data = np.load(test_binary_path, allow_pickle=True)

        seed = np.random.randint(1, 10e6)
        np.random.seed(seed)
        np.random.shuffle(test_data)

        return test_data


def label_bacteria_img(name, delim):
    word_label = name.split(delim)[0]
    if word_label == 'Escherichia.coli' : return np.array([1, 0])
    elif word_label == 'Lactobacillus.crispatus' : return np.array([0, 1])



# Nicely formatted time string
def hms_string(sec_elapsed):
    h = int(sec_elapsed / (60 * 60))
    m = int((sec_elapsed % (60 * 60)) / 60)
    s = sec_elapsed % 60
    return "{}:{:>02}:{:>05.2f}".format(h, m, s)
